# API Keys
OPENAI_API_KEY=your_openai_key_here

# Memory Mode Settings
# Set to true for low-memory environments (Mac M4 with 16GB RAM)
LOW_MEMORY_MODE=true
# Set to false to avoid Redis dependency
USE_REDIS=false

# Memory Limits
# Reduced from 100 for memory efficiency
SHORT_TERM_LIMIT=50
# Reduced from 1000 for memory efficiency
MID_TERM_LIMIT=250
# 12 hours (reduced from 86400)
CACHE_TTL=43200

# Model Settings
# Using local GGUF model by default
LLM_MODEL=local-gguf
RESPONSE_MODEL=local-gguf
DEFAULT_TEMPERATURE=0.7

# GGUF Model Settings
GGUF_MODEL_PATH=/Volumes/HomeX/yavuztopsever/LangGraph_Temp/models/gguf_llm/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf
GGUF_MAX_TOKENS=512
GGUF_CONTEXT_WINDOW=4096
GGUF_N_GPU_LAYERS=-1  # -1 means auto-detect

# Directory Settings
MEMORY_DIR=memory
DATA_DIR=data
VECTOR_DB_DIR=vector_store
GRAPH_DB_DIR=graph_store
LOGS_DIR=logs
STORAGE_DIR=storage

# Logging Settings
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Performance Settings
# Save state every 15 minutes (increase to reduce disk I/O)
STATE_SAVE_INTERVAL=900
