# Production configuration for LangGraph

# Core settings
core:
  debug: false
  log_level: INFO
  max_workers: 8
  timeout: 180  # seconds

# Model settings
models:
  llm:
    default_provider: openai
    providers:
      openai:
        model: gpt-4
        temperature: 0.3
        max_tokens: 2000
      anthropic:
        model: claude-2
        temperature: 0.7
        max_tokens: 1000
  embeddings:
    default_provider: sentence-transformers
    providers:
      sentence-transformers:
        model: all-MiniLM-L6-v2
        device: cuda
      openai:
        model: text-embedding-ada-002

# Storage settings
storage:
  vector:
    default_provider: chromadb
    providers:
      chromadb:
        persist_directory: /data/vector_store
        collection_name: production
      pinecone:
        index_name: prod-index
  cache:
    default_provider: redis
    providers:
      redis:
        host: ${REDIS_HOST}
        port: ${REDIS_PORT}
        db: 0
        password: ${REDIS_PASSWORD}
        ssl: true

# Workflow settings
workflow:
  max_retries: 3
  retry_delay: 10  # seconds
  timeout: 300
  max_concurrent_workflows: 20
  batch_size: 200
  parallel_execution: true

# Logging settings
logging:
  file: logs/prod.log
  error_file: /var/log/langgraph/error.log
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  max_size: 104857600  # 100MB
  backup_count: 10
  level: INFO
  console: false

# Monitoring settings
monitoring:
  prometheus:
    enabled: true
    port: 9090
  sentry:
    enabled: true
    dsn: ${SENTRY_DSN}
    environment: production
  metrics_port: 9090
  tracing:
    endpoint: ${JAEGER_ENDPOINT}
    sampler: 1.0
