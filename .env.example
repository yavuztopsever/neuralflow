# LLM Configuration
LLM_MODEL=deepseek-1.5b  # Default model optimized for 16GB RAM
# Available options: deepseek-8b, deepseek-1.5b, deepseek-7b, qwen-coder-7b

# Memory Management
MEMORY_MONITOR_ENABLED=true
MEMORY_HIGH_THRESHOLD=40  # Lower threshold for 16GB RAM
MEMORY_CRITICAL_THRESHOLD=55  # Lower critical threshold for 16GB RAM
MAX_LOADED_MODELS=1  # Only allow one model loaded at a time
MODEL_PRIORITY=deepseek-1.5b  # Prioritize smaller model for better performance

# M4-specific Model Settings
GGUF_N_THREADS=6  # Optimized for M4
GGUF_N_BATCH=256  # Conservative batch size for memory efficiency
GGUF_N_GPU_LAYERS=1  # Minimal GPU layers for memory efficiency
GGUF_MAX_TOKENS=1024  # Conservative token limit
GGUF_TOP_P=0.95
GGUF_TOP_K=40
GGUF_CONTEXT_WINDOW=4096  # Conservative context window

# Performance Settings
THREAD_POOL_ENABLED=true
MAX_THREADS=6  # Optimized for M4
DB_POOL_SIZE=2  # Reduced connection pool
DB_MAX_OVERFLOW=1  # Minimal overflow connections
DB_POOL_RECYCLE=900  # Recycle connections after 15 mins

# API Keys (Replace with your actual keys)
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key
HUGGINGFACE_API_KEY=your_huggingface_api_key

# Storage Paths
DATA_DIR=./src/storage/data
MODELS_DIR=./models
LOGS_DIR=./src/logs

# Vector Store Configuration
VECTOR_STORE_TYPE=pinecone  # Options: pinecone, weaviate, milvus
PINECONE_ENVIRONMENT=your_pinecone_environment
PINECONE_API_KEY=your_pinecone_api_key

# Cache Configuration
CACHE_TYPE=redis  # Options: redis, memcached, memory
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=your_redis_password  # Optional 